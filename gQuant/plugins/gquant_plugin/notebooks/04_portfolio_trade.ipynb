{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greenflow - Making Quantitative Analysis Faster\n",
    "\n",
    "## Background\n",
    "By definition, **Quantitative Finance** is the use of mathematical models and large datasets to analyze financial markets and securities, requiring massive computation to extract insight from the data. \n",
    "\n",
    "Many data science toolkits have been developed to help data scientists to manipulate the data. It starts with scalar number computations at the beginning. Later, the development of [Numpy](https://www.numpy.org) library helps to operate the numbers at vectors, and the popular [Pandas](https://pandas.pydata.org) library operates at a dataframe level. Manipulating data at a high level brings productivity gain for data scientists in quantitative finance.\n",
    "\n",
    "However, the amount of collected data is increasing exponentially over time.  Also, more and more machine learning and statistical models are being developed. As a result, data scientists are facing new challenges hard to deal with traditional data science libraries.\n",
    "\n",
    "It is very time-consuming for CPUs to crunch massive amount of data and compute the complicated data science models. Large data set requires distributed computation, which is too complicated for data scientists to adopt.\n",
    "\n",
    "As a consequence, the quantitative workflow has become more complicated than ever. It integrates massive data from different sources, requiring multiple iterations to obtain significative results. \n",
    "\n",
    "**greenflow** has been developed to address all these challenges by organizing dataframes into graphs. It introduces the idea of **dataframe-flow**, which manipulates dataframes at graph level. An **acyclic directed graph** is defined, where the nodes are dataframe processors and the edges are the directions of passing resulting dataframes.\n",
    "\n",
    "With a graph approach, quant's workflow is described at a high level, letting quant analysts address the complicated workflow challenge.\n",
    "\n",
    "It is GPU-accelerated by leveraging [RAPIDS.ai](https://rapids.ai) technology and has **Multi-GPU and Multi-Node support**.\n",
    "\n",
    "We can get orders of magnitude performance boosts compared to CPU. greenflow dataframe-flow is **dataframe agnostic**, and can flow:\n",
    "- Pandas dataframe, computed in the CPU.\n",
    "- cuDF dataframe, computed in the GPU and producing the same result but much faster.\n",
    "- dask_cuDF dataframe, being the computation automatically executed on multiple nodes and multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download example datasets\n",
    "\n",
    "Before getting started, let's download the example datasets if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already present. No need to re-download it.\n"
     ]
    }
   ],
   "source": [
    "! ((test ! -f './data/stock_price_hist.csv.gz' ||  test ! -f './data/security_master.csv.gz') && \\\n",
    "  cd .. && bash download_data.sh) || echo \"Dataset is already present. No need to re-download it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for running in Dask environment\n",
    "\n",
    "Let's start the Dask local cluster environment for distributed computation.\n",
    "\n",
    "Dask provides a web-based dashboard to help to track progress, identify performance issues, and debug failures. To learn more about Dask dashboard, just follow this [link](https://distributed.dask.org/en/latest/web.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:39757</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>100.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:39757' processes=2 threads=2, memory=100.00 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the Dask local cluster environment for distrubuted computation\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though our stock dataset is small enough to fit in a single 16G GPU, to show how to do distributed computation, we will split the dataframe into small pieces to be loaded by different workers in the cluster.\n",
    "\n",
    "Notice this step is need only if the dataset is not split in multiple files yet.\n",
    "\n",
    "First use this simple taskgraph to load data then sort it by the asset id and datatime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61924f0cdef24365b2086fdafc7b5acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'fileâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "from greenflow.dataframe_flow import TaskGraph\n",
    "task_graph = TaskGraph.load_taskgraph('../taskgraphs/sort_stocks.gq.yaml')\n",
    "input_cached, = task_graph.run()\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the sorted stock data into partitions and save it into csv files. Note, the data is slited in a way that the same asset belongs to the same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yi/Projects/demo_greenflow_install/notebooks/many-small/0.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/1.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/2.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/3.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/4.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/5.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/6.csv',\n",
       " '/home/yi/Projects/demo_greenflow_install/notebooks/many-small/7.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "num_partitions = 8\n",
    "\n",
    "os.makedirs('many-small', exist_ok=True)\n",
    "dd.from_pandas(input_cached.set_index('asset'), npartitions=num_partitions).reset_index().to_csv('many-small/*.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The toy example\n",
    "In this notebook, we will use a simple toy example to show how easy it is to accelerate the quant workflow in the GPU.\n",
    "\n",
    "To mimic the end-to-end quantitative analyst task, we are going to backtest a simple mean reversion trading strategy.\n",
    "\n",
    "The workflow can be divided into two steps. You can follow with me with an empty greenflow widget to build the TaskGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1684faac504943b287881bfe436fd78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "task_graph = TaskGraph()\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset to remove bad points and add return feature\n",
    "\n",
    "\n",
    "1. Load the 5000 end-of-day stocks CSV data into the dataframe and add rate of return feature to the dataframe.\n",
    "<img src='images/portfolio/add_indicator.gif'  align=\"center\" />\n",
    "\n",
    "2. Compute the average volume, min/max returns for each of the stocks\n",
    "<img src='images/portfolio/add_volume_min_max_return.gif'  align=\"center\" />\n",
    "\n",
    "3. Merge the features into one dataframe, clean up the data by removing low volume stocks and extreme rate of returns stocks.\n",
    "<img src='images/portfolio/filter_value.gif'  align=\"center\" />\n",
    "\n",
    "4. Create a composite node for this preprocess task\n",
    "<img src='images/portfolio/create_composite.gif'  align=\"center\" />\n",
    "\n",
    "### Apply simple mean reversion algorithm and run backtest\n",
    "\n",
    "1. Clean up the nodes for the backtest\n",
    "<img src='images/portfolio/clean_up_for_backtest.gif'  align=\"center\" />\n",
    "\n",
    "2. Compute the slow and fast exponential moving average and compute the trading signal based on it. Run backtesting and compute the returns from this strategy for each of the days and stock symbols. Run a simple portfolio optimization by averaging the stocks together for each of the trading days. Compute the sharpe ratio and cumulative return results.\n",
    "<img src='images/portfolio/backtest.gif'  align=\"center\" />\n",
    "\n",
    "3. Change the `slow`, `fast` parameters for the trading stratiges and re-run the backtest\n",
    "<img src='images/portfolio/change_parameters.gif'  align=\"center\" />\n",
    "\n",
    "4. Switch to run the backtest in a distributed environment by Dask\n",
    "<img src='images/portfolio/run_dask_trade.gif'  align=\"center\" />\n",
    "\n",
    "5. As a reference, switch to run the backtest in a CPU environment by Pandas\n",
    "<img src='images/portfolio/run_pandas.gif'  align=\"center\" />\n",
    "\n",
    "The whole workflow is organized into a TaskGraph file, which is described in a **gq.yaml** file.\n",
    "\n",
    "The same taskgraphs are saved in the `taskgraphs` directories. The whole workflow can be organized into a computation graph, which is described in a **yaml** file. \n",
    "\n",
    "Here is snippet of the yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- conf:\n",
      "    file: notebooks/data/stock_price_hist.csv.gz\n",
      "    path: notebooks/many-small\n",
      "  id: stock_data\n",
      "  inputs: {}\n",
      "  module: greenflow_gquant_plugin.dataloader\n",
      "  type: CsvStockLoader\n",
      "- conf:\n",
      "    input:\n",
      "    - sort_node.in\n",
      "    output:\n",
      "    - drop_columns.out\n",
      "    subnode_ids:\n",
      "    - value_filter\n",
      "    subnodes_conf:\n",
      "      value_filter:\n",
      "        conf:\n",
      "        - column: min_return\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "!head -n 18 ../taskgraphs/portfolio_trade.gq.yaml\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lode the preprocess TaskGraph by `load_taskgraph` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91e0a7be3bf4e69973965709e5eff9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'fileâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph = TaskGraph.load_taskgraph('../taskgraphs/preprocess.gq.yaml')\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lode the whole TaskGraph by `load_taskgraph` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce4096ee9dd4ec78d929077650a9899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'fileâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph = TaskGraph.load_taskgraph('../taskgraphs/portfolio_trade.gq.yaml')\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this toy example in a Dask distributed environment is super easy, as greenflow operates at dataframe level.\n",
    "\n",
    "We just need to swap cuDF dataframes to **dask_cuDF** dataframes. Try to connect the `preprocess` node to the `Dask dataframe` output port in the `stock data` node.\n",
    "\n",
    "Similarly, to see how fast the GPU acceleration is, we can swtich to CPU computation environment by connecting to the `Pandas dataframe` output port.\n",
    "\n",
    "## Benchmarks\n",
    "\n",
    "While running this notebook, we have obtained the following results:\n",
    "\n",
    "- 181.00 seconds to run in CPU (Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz).\n",
    "-   9.06 seconds to run in GPU (NVIDIA v100).\n",
    "\n",
    "We get ~20x speed up by using GPU and GPU dataframes, compared to CPU and CPU dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greenflow Task Node \n",
    "\n",
    "Each node is composed of:\n",
    "- a unique id,\n",
    "- a node type, \n",
    "- configuration parameters\n",
    "- from zero to many input nodes ids.\n",
    "\n",
    "greenflow's `load_taskgraph` takes this yaml file, and wires it into a graph.\n",
    "\n",
    "greenflow implementation includes some common nodes, useful for quantitative finance. With the help of [Numba](https://numba.pydata.org) library, we have implemented more than 30 technical indicators used in computing trading signals. All of them computed in the GPU.\n",
    "\n",
    "However, greenflow's goal is not to be comprehensive for quant applications. It provides a framework that is easy for anyone to implement his own nodes in the greenflow.\n",
    "\n",
    "\n",
    "Data scientists only need to override five methods in the parent class `Node`:\n",
    "- `init`\n",
    "- `meta_setup`\n",
    "- `ports_setup`\n",
    "- `conf_schema`\n",
    "- `process`\n",
    "\n",
    "`init` method is usually used to define the required column names\n",
    "\n",
    "`ports_setup` defines the input and output ports for the node\n",
    "\n",
    "`meta_setup` method is used to calculate the output meta name and types.\n",
    "\n",
    "`conf_schema` method is used to define the JSON schema for the node conf so the client can generate the proper UI for it.\n",
    "\n",
    "`process` method takes input dataframes and computes the output dataframe. \n",
    "\n",
    "In this way, dataframes are strongly typed, and errors can be detected early before the time-consuming computation happens.\n",
    "\n",
    "Here is the code example for implementing `MaxNode`, which is to compute the maximum value for a specified column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenflow.dataframe_flow import Node\n",
    "from greenflow_gquant_plugin._port_type_node import _PortTypesMixin\n",
    "from greenflow.dataframe_flow.portsSpecSchema import ConfSchema\n",
    "\n",
    "\n",
    "class MaxNode(Node, _PortTypesMixin):\n",
    "\n",
    "    def init(self):\n",
    "        _PortTypesMixin.init(self)\n",
    "        self.INPUT_PORT_NAME = 'in'\n",
    "        self.OUTPUT_PORT_NAME = 'out'\n",
    "\n",
    "    def ports_setup(self):\n",
    "        return _PortTypesMixin.ports_setup(self)\n",
    "\n",
    "    def conf_schema(self):\n",
    "        json = {\n",
    "            \"title\": \"Maximum Value Node configure\",\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Compute the maximum value of the key column\",\n",
    "            \"properties\": {\n",
    "                \"column\":  {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"column to calculate the maximum value\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"column\"],\n",
    "        }\n",
    "        input_meta = self.get_input_meta()\n",
    "        if self.INPUT_PORT_NAME in input_meta:\n",
    "            col_from_inport = input_meta[self.INPUT_PORT_NAME]\n",
    "            enums = [col for col in col_from_inport.keys()]\n",
    "            json['properties']['column']['enum'] = enums\n",
    "            ui = {}\n",
    "            return ConfSchema(json=json, ui=ui)\n",
    "        else:\n",
    "            ui = {\n",
    "                \"column\": {\"ui:widget\": \"text\"}\n",
    "            }\n",
    "            return ConfSchema(json=json, ui=ui)\n",
    "\n",
    "    def process(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute the maximum value of the key column which is defined in the\n",
    "        `column` of the node's conf\n",
    "\n",
    "        Arguments\n",
    "        -------\n",
    "         inputs: list\n",
    "            list of input dataframes.\n",
    "        Returns\n",
    "        -------\n",
    "        dataframe\n",
    "        \"\"\"\n",
    "        input_df = inputs[self.INPUT_PORT_NAME]\n",
    "        max_column = self.conf['column']\n",
    "        volume_df = input_df[[max_column,\n",
    "                              \"asset\"]].groupby([\"asset\"]).max().reset_index()\n",
    "        volume_df.columns = ['asset', max_column]\n",
    "        return {self.OUTPUT_PORT_NAME: volume_df}\n",
    "\n",
    "    def meta_setup(self):\n",
    "        cols_required = {\"asset\": \"int64\"}\n",
    "        if 'column' in self.conf:\n",
    "            retention = {self.conf['column']: \"float64\",\n",
    "                         \"asset\": \"int64\"}\n",
    "            return _PortTypesMixin.retention_meta_setup(self,\n",
    "                                                        retention,\n",
    "                                                        required=cols_required)\n",
    "        else:\n",
    "            retention = {\"asset\": \"int64\"}\n",
    "            return _PortTypesMixin.retention_meta_setup(self,\n",
    "                                                        retention,\n",
    "                                                        required=cols_required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case that there is no direct dataframe method for a particular logic, a Numba GPU kernel can be used to implement it. Some examples of customized GPU kernels in Numba can be found [here](https://github.com/rapidsai/greenflow/blob/master/notebooks/05_customize_nodes.ipynb).\n",
    "\n",
    "If we use customized GPU kernel functions inside the `process` method to process the dataframe instead of _normal_ dataframe API functions calls,  we need to add `self.delayed_process = True` in the `meta_setup` method to let greenflow handle the dask graph integration problem. If we use  _normal_ dataframe API functions inside the `process` method, nothing needs to be done as `self.delayed_process = False` by default.bgreenflow automatically handles the complication of including a customized GPU kernel node into the Dask computation graph.\n",
    "\n",
    "Note, we set `self.delayed_process = True` for the `SortNode`. So th sort is performed at the Dask data partition level instead of sorting it globally. This has a benefits of guranteeing the sortting doens't pollute the data partition allocation, as sometimes we want to make sure the data partition remain the same during the distributed computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the TaskGraph programmatically\n",
    "\n",
    "\n",
    "To run the TaskGraph programmatically , we can specifiy a list of output ports to the TaskGraph `run` method. The `profile` flag can be used to see the computation time spent on each of the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:stock_data process time:4.120s\n",
      "id:preprocess process time:0.879s\n",
      "id:sort_after process time:0.055s\n",
      "id:exp_mean_reversion process time:0.796s\n",
      "id:backtest process time:0.001s\n",
      "id:portfolio_opt process time:0.012s\n",
      "id:sharpe_ratio process time:0.001s\n",
      "id:cumulative_return process time:0.016s\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "\n",
    "o_gpu = task_graph.run(\n",
    "            outputs=['sharpe_ratio.sharpe_out', 'cumulative_return.cum_return','stock_data.cudf_out', 'preprocess.drop_columns@out'], profile=True)\n",
    "gpu_strategy_cached = o_gpu['preprocess.drop_columns@out'] \n",
    "gpu_input_cached = o_gpu['stock_data.cudf_out']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`o_gpu` will contain the outputs of four nodes: `sharpe_ratio`, `cumlative_return`, `stock_data`, `preprocess`.\n",
    "\n",
    "Similarly, the output from `stock_data` and `preprocess` nodes will be cached stored in `gpu_input_cached` and `strategy_cached` variables for later use. \n",
    "\n",
    "We can check how many of the stocks are filtered out by preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5052 stocks in original dataset.\n",
      "1558 stocks remaining after filtering.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} stocks in original dataset.\".format(len(gpu_input_cached['asset'].unique())))\n",
    "print(\"{} stocks remaining after filtering.\".format(len(gpu_strategy_cached['asset'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be shown in IPython Rich display by turnning on the `formatted` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca90df245c254e4ba78ae8ac9bd404ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(layout=Layout(border='1px solid black'), outputs=({'output_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph.run(\n",
    "            outputs=['sharpe_ratio.sharpe_out', 'cumulative_return.cum_return','preprocess.drop_columns@out'], formated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This toy strategy gets a Sharpe ratio 0.338 without considering the transaction cost. Nice! \n",
    "\n",
    "[bqplot](https://github.com/bloomberg/bqplot) library is used to visualize the backtesting results in the JupyterLab notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0336888fa940d4903d91ce87244888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Cumulative return', orientation='vertical', scale=LinearScale(), side='left'), Axis(lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the function to format the plots\n",
    "def plot_figures(outputs):\n",
    "    # format the figures\n",
    "    figure_width = '1200px'\n",
    "    figure_height = '400px'\n",
    "    sharpe_number = outputs[0]\n",
    "    cum_return = outputs[1]\n",
    "    cum_return.layout.height = figure_height\n",
    "    cum_return.layout.width = figure_width\n",
    "    cum_return.title = 'P & L %.3f' % (sharpe_number)\n",
    "    return cum_return\n",
    "\n",
    "plot_figures(o_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greenflow TaskGraph can be evaluated by overwritting any of the Node's parameters. E.g. we can change the parameters to filter out the stocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants for the data filters.\n",
    "# If using a GPU of 32G memory, you can safely \n",
    "# set the `min_volume` to 5.0\n",
    "min_volume = 10.0\n",
    "min_rate = -10.0\n",
    "max_rate = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:stock_data process time:4.021s\n",
      "id:preprocess process time:0.702s\n",
      "id:sort_after process time:0.130s\n",
      "id:exp_mean_reversion process time:0.079s\n",
      "id:backtest process time:0.003s\n",
      "id:portfolio_opt process time:0.017s\n",
      "id:sharpe_ratio process time:0.001s\n",
      "id:cumulative_return process time:0.015s\n",
      "5052 stocks in original dataset.\n",
      "4405 stocks remaining after filtering.\n",
      "CPU times: user 4.82 s, sys: 725 ms, total: 5.54 s\n",
      "Wall time: 5.36 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4227850521c94892a4fe3c2b94183811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Cumulative return', orientation='vertical', scale=LinearScale()), Axis(label='Time', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "o_gpu = task_graph.run(\n",
    "            outputs=['sharpe_ratio.sharpe_out', 'cumulative_return.cum_return', 'stock_data.cudf_out', 'preprocess.drop_columns@out'],    \n",
    "            replace={'preprocess': {\"conf\": {\n",
    "                                        \"subnodes_conf\": {\n",
    "                                            \"value_filter\": {\n",
    "                                                \"conf\": [{\"column\": \"average_volume\", \"min\": min_volume},\n",
    "                                                         {\"column\": \"max_return\", \"max\": max_rate},\n",
    "                                                         {\"column\": \"min_return\", \"min\": min_rate}]\n",
    "                                            }\n",
    "                                         },\n",
    "                                        \"taskgraph\": \"taskgraphs/preprocess.gq.yaml\",\n",
    "                                        \"input\": [\"sort_node.in\"],\n",
    "                                        \"output\": [\"drop_columns.out\"]\n",
    "                                    }}\n",
    "                    }, profile=True)\n",
    "\n",
    "gpu_input_cached = o_gpu['stock_data.cudf_out']  \n",
    "gpu_strategy_cached = o_gpu['preprocess.drop_columns@out'] \n",
    "print(\"{} stocks in original dataset.\".format(len(gpu_input_cached['asset'].unique())))\n",
    "print(\"{} stocks remaining after filtering.\".format(len(gpu_strategy_cached['asset'].unique())))\n",
    "plot_figures(o_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, `preprocess` node is a composite node that use a seperate TaskGraph as input and output. Any of the node inside the composite node TaskGraph configuration can be overridden as shown in the example. We change the `filter_value` node configuration  inside the composite node to filter out the stocks that are not suitable for backtesting. It will discard stocks according to the values stored in `min_volume`, `min_rate`, and `max_rate` variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to compare the performance difference between CPU and GPU. The same computation graph can be used to flow the CPU Pandas dataframe with one change that\n",
    "the preprocess node need to get input from the Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:stock_data process time:64.310s\n",
      "id:preprocess process time:22.999s\n",
      "id:sort_after process time:1.303s\n",
      "id:exp_mean_reversion process time:6.645s\n",
      "id:backtest process time:0.043s\n",
      "id:portfolio_opt process time:0.448s\n",
      "id:sharpe_ratio process time:0.001s\n",
      "id:cumulative_return process time:0.014s\n",
      "CPU times: user 1min 23s, sys: 14.5 s, total: 1min 38s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "o_cpu = task_graph.run(\n",
    "            outputs=['sharpe_ratio.sharpe_out', 'cumulative_return.cum_return'],    \n",
    "            replace={'preprocess': {\"inputs\": {\"sort_node@in\": \"stock_data.pandas_out\"}}}, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2c4dadba0148d5b9294a42ec0689a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Cumulative return', orientation='vertical', scale=LinearScale(), side='left'), Axis(lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_figures(o_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It produces the same result as the single GPU version but a lot slower.\n",
    "\n",
    "While running this notebook, we have obtained the following results:\n",
    "\n",
    "- 181.00 seconds to run in CPU (Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz).\n",
    "-   9.06 seconds to run in GPU (NVIDIA v100).\n",
    "\n",
    "We get ~20x speed up by using GPU and GPU dataframes, compared to CPU and CPU dataframes.\n",
    "\n",
    "Note, the input nodes load the dataframes from the cache variables to save the disk IO time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed computation is turned on by changing the preprocess node's input dataframe to dask dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:stock_data process time:0.115s\n",
      "id:preprocess process time:7.280s\n",
      "id:backtest process time:0.013s\n",
      "id:portfolio_opt process time:0.036s\n",
      "id:sharpe_ratio process time:0.328s\n",
      "id:cumulative_return process time:0.353s\n",
      "CPU times: user 3.85 s, sys: 377 ms, total: 4.22 s\n",
      "Wall time: 14.4 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf0fa6953074fad9f9bd7e7fa08872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(axes=[Axis(label='Cumulative return', orientation='vertical', scale=LinearScale()), Axis(label='Time', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "o_dask = task_graph.run(\n",
    "            outputs=['sharpe_ratio.sharpe_out', 'cumulative_return.cum_return'],    \n",
    "            replace={'preprocess': {\"inputs\": {\"sort_node@in\": \"stock_data.dask_cudf_out\"}}}, profile=True)\n",
    "plot_figures(o_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it produces the same results. However, the performance is not better than in the single GPU scenarios.\n",
    "\n",
    "Distributed computation only makes sense if we have a very large dataset that cannot be fit into one GPU.\n",
    "\n",
    "In this example, the dataset is small enough to be loaded into a single GPU. The between-GPU communication overhead dominates in the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy parameter search\n",
    "Quantitative analysts often need to explore different parameters for their trading strategy.\n",
    "\n",
    "greenflow speeds up this iterative exploration process by using cached dataframes and sub-graphs evaluation.\n",
    "\n",
    "To find the optimal parameters for this toy mean reversion strategy, we only need the dataframe from `sort_2` node, which is cached in the `gpu_strategy_cached` variable.\n",
    "\n",
    "Because the GPU computation is so fast, we can make the parameter exploration interactive in the JupyterLab notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f5d401388b40648ce422e3973f4d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntRangeSlider(value=(10, 30), continuous_update=False, description='MA:', max=6â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "para_selector = widgets.IntRangeSlider(value=[10, 30],\n",
    "                                       min=3,\n",
    "                                       max=60,\n",
    "                                       step=1,\n",
    "                                       description=\"MA:\",\n",
    "                                       disabled=False,\n",
    "                                       continuous_update=False,\n",
    "                                       orientation='horizontal',\n",
    "                                       readout=True)\n",
    "\n",
    "\n",
    "def para_selection(*stocks):\n",
    "    with out:\n",
    "        para1 = para_selector.value[0]\n",
    "        para2 = para_selector.value[1]\n",
    "        o = task_graph.run(\n",
    "                outputs=['sharpe_ratio.sharpe_out', 'cumulative_return.cum_return'],\n",
    "                replace={'exp_mean_reversion': {'conf':  {'fast': para1,\n",
    "                                                          'slow': para2}},\n",
    "                         'preprocess': {\"load\": {\"drop_columns@out\": gpu_strategy_cached},\n",
    "                                        \"conf\": {\n",
    "                                        \"subnodes_conf\": {\n",
    "                                            \"value_filter\": {\n",
    "                                                \"conf\": [{\"column\": \"average_volume\", \"min\": min_volume},\n",
    "                                                         {\"column\": \"max_return\", \"max\": max_rate},\n",
    "                                                         {\"column\": \"min_return\", \"min\": min_rate}]\n",
    "                                            }\n",
    "                                         },\n",
    "                                        \"taskgraph\": \"taskgraphs/preprocess.gq.yaml\",\n",
    "                                        \"input\": [\"sort_node.in\"],\n",
    "                                        \"output\": [\"drop_columns.out\"]\n",
    "                                    }}})\n",
    "\n",
    "        figure_combo = plot_figures(o)\n",
    "        w.children = (w.children[0], figure_combo,)\n",
    "\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "para_selector.observe(para_selection, 'value')\n",
    "selectors = widgets.HBox([para_selector])\n",
    "w = widgets.VBox([selectors])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b093c4f26244fdf831e4b1507dec36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
